from __future__ import print_function
import os
import numpy as np
np.random.seed(1337)
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
# fix random seed for reproducibility
numpy.random.seed(7)

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding
from keras.models import Model
import sys
import csv


top_words_considered=10000
max_sentence_length=15

validation_split=0.2
file='stanford q&a.csv'
word_vector_file='glove.6B.300d.txt'


print ("Loading word vectors.....")
embeddings_index = {}
f = open(word_vector_file)

for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = (1+coefs)/(2.0)


f.close()
print ("Word Vectors Loaded!!")
print('Found %s word vectors.' % len(embeddings_index))

word_vector_length=len(embeddings_index.get('dog'))


print ("Loading the data file....")
f=open(file)
f=csv.reader(f)
texts=[]
c=[]
for i in f:
   texts.append(i[0])
#   c.append(i[1])


#labels=[]
#for i in c:
#  labels.append((np.where(i==np.unique(c)))[0][0]) 
     

#labels_index={}
#for i in np.unique(c):
#  for j in np.arange(len(c)):
#    if (i==c[j]):
#      labels_index[i]=labels[j]


print ("Datafile loaded!!")


#c=[]
MAX_SEQUENCE_LENGTH = max_sentence_length
MAX_NB_WORDS = top_words_considered
EMBEDDING_DIM = word_vector_length
VALIDATION_SPLIT = validation_split



tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))


index_word={}
for i,j in word_index.items():
  index_word[j]=i



data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

final_data=[]
for i in data:
  t=[]
  for j in i:
    if (j==0):
      t.append(np.zeros(300,))
    elif (embeddings_index.get(index_word[j]) is None):
      t.append(np.ones(300,))
    else:
      t.append(embeddings_index.get(index_word[j]))
  final_data.append(t)



final_data=np.array(final_data).reshape(len(final_data),MAX_SEQUENCE_LENGTH,EMBEDDING_DIM)

#labels = to_categorical(np.asarray(labels))
print('Shape of data tensor:', data.shape)
#print('Shape of label tensor:', labels.shape)

# split the data into a training set and a validation set
indices = np.arange(final_data.shape[0])
np.random.shuffle(indices)
data = final_data[indices]
#labels = labels[indices]
nb_validation_samples = int(VALIDATION_SPLIT * final_data.shape[0])

x_train = data[:-nb_validation_samples]
#y_train = labels[:-nb_validation_samples]
x_val = data[-nb_validation_samples:]
#y_val = labels[-nb_validation_samples:]

print('Preparing embedding matrix.')

# prepare embedding matrix
#nb_words = min(MAX_NB_WORDS, len(word_index))
#embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))
#for word, i in word_index.items():
#  if i >= MAX_NB_WORDS:
#    continue
#  embedding_vector = embeddings_index.get(word)
#  if embedding_vector is not None:
#    embedding_matrix[i] = embedding_vector



# load pre-trained word embeddings into an Embedding layer
# note that we set trainable = False so as to keep the embeddings fixed
#embedding_layer = Embedding(nb_words,
#                            EMBEDDING_DIM,
#                            weights=[embedding_matrix],
#                            input_length=MAX_SEQUENCE_LENGTH,
#                            trainable=False)

#print('Training model.')


ip=Input(shape=(MAX_SEQUENCE_LENGTH,EMBEDDING_DIM))
encoder=LSTM(500)(ip)
decoder=RepeatVector(MAX_SEQUENCE_LENGTH)(encoder)
decoder=LSTM(50,return_sequences=True)(decoder)
decoder=TimeDistributed(Dense(EMBEDDING_DIM,activation='sigmoid'))(decoder)

model=Model(input=ip,output=decoder)

model.compile(loss='cosine_proximity', optimizer='adam', metrics=['accuracy'])
print(model.summary())
model.fit(x_train, x_train, nb_epoch=20,validation_data=(x_val,x_val))
# Final evaluation of the model
scores = model.evaluate(x_val, x_val, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))



from random import randint
sent=[texts[randint(0,len(texts))]]
sent=['cant wait']
sent[0]
sequences = tokenizer.texts_to_sequences(sent)
data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
model.predict(np.array(data))


idx=np.argmax(model.predict(np.array(data)))

for cat,i in labels_index.items():
  if (i==idx):
     sent[0]
     print (cat)








